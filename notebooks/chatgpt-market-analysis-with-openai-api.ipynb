{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intallations\n",
        "\n",
        "\n",
        "This code installs several Python packages (tiktoken, cohere, openai, langchain, chromadb, and unstructured) using the pip package manager. These packages offer various functionalities, such as token counting (tiktoken), language analysis (cohere, langchain), access to the OpenAI API (openai), and other utility functions related to working with unstructured data (chromadb, unstructured). These installations enable users to leverage different tools for natural language processing, data analysis, and accessing external APIs."
      ],
      "metadata": {
        "id": "zXuY4vLbY_ER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken\n",
        "!pip install -q cohere\n",
        "!pip install -q openai\n",
        "!pip install -q langchain\n",
        "!pip install -q chromadb\n",
        "!pip install -q unstructured"
      ],
      "metadata": {
        "id": "zUtOR_KVef-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6e19f4c-df62-43a1-d80a-24da58cc3194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m313.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.1/212.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required Functions"
      ],
      "metadata": {
        "id": "4tlAUWOCZB_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code provides various utility functions for interacting with the OpenAI GPT model in a chat-based application. It includes functionalities such as formatting and printing chat completion messages, calculating daily usage costs of the OpenAI API, token counting, and data processing.\n",
        "\n",
        "**Functions:**\n",
        "\n",
        "1. **print_plain_text(chat_completion_message, line_width=120):**\n",
        "\n",
        "* Formats and prints chat completion messages, considering bold, italic, bullet points, and numbered points.\n",
        "* Returns the clean text without HTML tags.\n",
        "\n",
        "2. **calculate_daily_usage(api_key):**\n",
        "\n",
        "* Calculates the used balance in dollars for the current day based on the OpenAI API usage.\n",
        "* Requires an API key for authentication.\n",
        "\n",
        "3. **count_tokens_generic(input_string):**\n",
        "\n",
        "* Counts the number of tokens in a given input string using the NLTK word tokenizer.\n",
        "\n",
        "4. **trim_tokens_nltk(input_string, max_tokens=10000):**\n",
        "\n",
        "* Trims the input string to have a maximum specified number of tokens using the NLTK word tokenizer.\n",
        "* Provides an estimated number of tokens after trimming.\n",
        "\n",
        "5. **load_data(filename):**\n",
        "\n",
        "* Loads data from a text file specified by the filename.\n",
        "\n",
        "6. **upload_file():**\n",
        "\n",
        "* Allows users to upload a dataset file and choose to consider the entire dataset or a specific column for analysis.\n",
        "* Supports .txt, .xlsx, and .csv file formats.\n",
        "* Provides an estimated number of tokens in the uploaded data.\n",
        "\n",
        "\n",
        "**Usage:**\n",
        "* Users can interactively upload datasets, choose analysis options, and prompt the OpenAI model for responses.\n",
        "* The code facilitates user-friendly formatting of chat completion messages and tracking daily usage costs.\n",
        "* This set of utilities enhances the efficiency of utilizing OpenAI's GPT model in a chat-based application."
      ],
      "metadata": {
        "id": "a71VQURjOtAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import datetime\n",
        "import openai\n",
        "import nltk\n",
        "import re\n",
        "import textwrap\n",
        "import os\n",
        "import math\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import files\n",
        "from openai import OpenAI\n",
        "from decimal import Decimal\n",
        "from io import open\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_h8g0S8lqF4",
        "outputId": "45f8dfba-b923-4637-deb3-4d6061147337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_plain_text(chat_completion_message, line_width=120):\n",
        "    # Split the content into paragraphs\n",
        "    paragraphs = chat_completion_message.content.split('\\n\\n')\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        # Wrap lines within each paragraph\n",
        "        wrapped_lines = textwrap.wrap(paragraph, width=line_width)\n",
        "\n",
        "        # Process and print each wrapped line\n",
        "        for line in wrapped_lines:\n",
        "            # Check for bold and italic formatting\n",
        "            formatted_line = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\033[1m\\1\\033[0m', line)  # Bold\n",
        "            formatted_line = re.sub(r'\\*(.*?)\\*', r'\\033[3m\\1\\033[0m', formatted_line)  # Italic\n",
        "\n",
        "            # Check for bullet points\n",
        "            if formatted_line.strip().startswith(\"- \"):\n",
        "                formatted_line = re.sub(r'^- (.*)$', r'\\033[1m•\\033[0m \\1', formatted_line)\n",
        "\n",
        "            # Check for numbered points\n",
        "            elif re.match(r'^\\d+\\. ', formatted_line):\n",
        "                formatted_line = re.sub(r'^(\\d+)\\. (.*)$', r'\\033[1m\\1.\\033[0m \\2', formatted_line)\n",
        "\n",
        "            # Print the formatted line\n",
        "            print(formatted_line)\n",
        "\n",
        "    clean_text = re.sub(\"<[^>]*>\", \"\", chat_completion_message.content).strip()\n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def calculate_daily_usage(api_key):\n",
        "    # API headers\n",
        "    headers = {'Authorization': f'Bearer {api_key}'}\n",
        "\n",
        "    # API endpoint\n",
        "    url = 'https://api.openai.com/v1/usage'\n",
        "\n",
        "    # Get the current date\n",
        "    current_date = datetime.date.today()\n",
        "\n",
        "    # Parameters for API request\n",
        "    params = {'date': current_date.strftime('%Y-%m-%d')}\n",
        "\n",
        "    # Send API request and get response\n",
        "    response = requests.get(url, headers=headers, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        usage_data = response.json().get('data', [])\n",
        "\n",
        "        # Calculate used balance in dollars\n",
        "        total_tokens = 0\n",
        "        for data in usage_data:\n",
        "            total_tokens += data['n_generated_tokens_total'] + data['n_context_tokens_total']\n",
        "\n",
        "        used_balance_in_tokens = total_tokens\n",
        "        used_balance_in_dollars = (used_balance_in_tokens * 0.002) / 1000\n",
        "\n",
        "        return used_balance_in_dollars\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}, {response.json()}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def count_tokens_generic(input_string):\n",
        "    # Download the Punkt tokenizer models if not already downloaded\n",
        "    # nltk.download('punkt')\n",
        "\n",
        "    # Tokenize the input string using the nltk word tokenizer\n",
        "    tokens = nltk.word_tokenize(input_string)\n",
        "\n",
        "    return len(tokens)\n",
        "\n",
        "\n",
        "def trim_tokens_nltk(input_string, max_tokens=10000):\n",
        "    # Tokenize the input string using the nltk word tokenizer\n",
        "    tokens = nltk.word_tokenize(input_string)\n",
        "\n",
        "    if len(tokens) > max_tokens:\n",
        "        # Truncate the list of tokens to the maximum number of tokens\n",
        "        trimmed_tokens = tokens[:max_tokens]\n",
        "        # Join the tokens back into a string\n",
        "        input_string = ' '.join(trimmed_tokens)\n",
        "\n",
        "        estimated_tokens = count_tokens_generic(input_string)\n",
        "        print(f\"\\nEstimated number of tokens after trimming: {estimated_tokens}\")\n",
        "\n",
        "    return input_string\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        dataset = f.read()\n",
        "    return dataset\n",
        "\n",
        "def upload_file():\n",
        "    print('Please select \"Choose File\" to select a dataset from your hard disk for upload and analysis: \\n')\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"File upload canceled.\")\n",
        "        return None\n",
        "\n",
        "    file_name = next(iter(uploaded))\n",
        "    print(f\"Uploaded file: {file_name}\")\n",
        "\n",
        "    if file_name.lower().endswith(('.txt', '.text')):\n",
        "        # If it's a text file, load it directly\n",
        "        return load_data(file_name)\n",
        "    elif file_name.lower().endswith(('.xlsx', '.csv')):\n",
        "        # If it's an Excel or CSV file, convert to text\n",
        "        df = pd.read_excel(file_name) if file_name.lower().endswith('.xlsx') else pd.read_csv(file_name)\n",
        "        print('\\n----------------------------------------------------------------------------------------------------------------\\n')\n",
        "        print('Do you want to consider a single column of the dataset?')\n",
        "        print('Type \"yes\" if you want to choose a single column, and type \"no\" if you wish to use the entire dataset:')\n",
        "        trim_data_bool = input()\n",
        "\n",
        "        print('\\n----------------------------------------------------------------------------------------------------------------\\n')\n",
        "        if(trim_data_bool == 'yes'):\n",
        "            print('Please type the title of the column you wish to consider for analysis:')\n",
        "            single_feature = input()\n",
        "            if(single_feature in df.columns):\n",
        "                df = df[single_feature]\n",
        "                print(f\"\\nDataset uploaded and column '{single_feature}' is selected for analysis.\")\n",
        "            else:\n",
        "                df = df\n",
        "                print(f\"\\n'{single_feature}' is not found in the column titles of the dataset. \\nTherefore, entire dataset uploaded and ready for analysis.\")\n",
        "        else:\n",
        "            df = df\n",
        "            print(f\"Entire dataset uploaded and ready for analysis.\")\n",
        "\n",
        "        #print('\\n----------------------------------------------------------------------------------------------------------------\\n')\n",
        "        text_data = df.to_string(index=False)\n",
        "\n",
        "        estimated_tokens = count_tokens_generic(text_data)\n",
        "        print(f\"\\nEstimated number of tokens: {estimated_tokens}\")\n",
        "\n",
        "        return text_data\n",
        "    else:\n",
        "        print(\"Unsupported file format. Please upload a .txt, .xlsx, or .csv file.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "qYM9o5e0Z8Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below defines a main loop function for an interactive chat-based application using the OpenAI GPT-4 model. The loop allows users to prompt the model with questions related to a provided dataset, analyze market dynamics, and store the conversation history. Here's a breakdown of the key features:\n",
        "\n",
        "**Features:**\n",
        "\n",
        "1. **User Guide:**\n",
        "\n",
        "* Provides a user-friendly guide with instructions on how to interact with the chat application.\n",
        "* Informs users about commands to exit, quit, or delete chat history.\n",
        "\n",
        "2. **Chat History Handling:**\n",
        "\n",
        "* Checks for the existence of a chat history file.\n",
        "* Asks the user if they want to delete existing chat history.\n",
        "* Deletes or loads chat history accordingly.\n",
        "\n",
        "3. **Interactive Loop:**\n",
        "\n",
        "* Prompts the user to input questions or prompts.\n",
        "* Handles exit commands to stop the chat loop.\n",
        "\n",
        "4. **System and User Roles:**\n",
        "\n",
        "* Defines roles for the system (market analyzer) and the user.\n",
        "* Incorporates provided dataset and chat history into the system role.\n",
        "* Generates a user role based on the user's prompt.\n",
        "\n",
        "5. **OpenAI GPT-4 Interaction:**\n",
        "\n",
        "* Utilizes the OpenAI GPT-4 model to generate responses.\n",
        "* Employs the Chat API for chat-based completions.\n",
        "\n",
        "6. **Output Formatting:**\n",
        "\n",
        "* Prints the analyzer's output in a formatted manner using the print_plain_text function.\n",
        "\n",
        "7. **Storage of Prompts and Outputs:**\n",
        "\n",
        "* Stores user prompts and model-generated outputs in a dictionary (prompts_and_outputs).\n",
        "* Appends prompt-response pairs to the chat history file.\n",
        "\n",
        "**Usage:**\n",
        "* Users can interactively provide prompts and receive responses from the GPT-4 model.\n",
        "* The chat history is maintained and can be deleted or loaded based on user preference.\n",
        "* Prompts and responses are stored for future reference.\n",
        "\n",
        "This main loop function enhances the user experience in querying and analyzing a dataset using the OpenAI GPT-4 model within a chat-based application."
      ],
      "metadata": {
        "id": "kSh1Rc8gQi67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for the main loop\n",
        "def main_loop(api_key, chat_history_file, dataset):\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    prompts_and_outputs = {}\n",
        "\n",
        "    print('\\n\\033[1m------------------------------------------------------------------------------------------------------------------------\\033[0m \\n')\n",
        "    print(\"\\033[1mUser Guide\\033[0m\")\n",
        "    print('1) Please type your prompt to ask questions about the dataset.')\n",
        "    print('2) You can use \"q\", \"quit\", or \"exit\" commands to stop the code from asking for prompts.')\n",
        "    print('3) If asked for deleting the previous chat history or not, typing \"yes\" will delete it and then the model')\n",
        "    print('   will not consider previous chat data in its answers. Otherwise, type \"no\" for using the previous chat data.')\n",
        "    print('   (These words are case-sensitive)')\n",
        "    print('\\n\\033[1m------------------------------------------------------------------------------------------------------------------------\\033[0m \\n')\n",
        "\n",
        "    # Check if the file exists\n",
        "    if os.path.exists(chat_history_file):\n",
        "        # Ask the user if they want to delete the existing data\n",
        "        user_input = input(\"Previous chat history exists. Do you want to delete it? (yes/no): \").lower()\n",
        "\n",
        "        if user_input == 'yes':\n",
        "            # Delete existing data\n",
        "            open(chat_history_file, 'w').close()\n",
        "            print(\"Existing chat history deleted.\")\n",
        "            chat_history_string = \"\"\n",
        "        else:\n",
        "            # Load existing data\n",
        "            with open(chat_history_file, 'r') as f:\n",
        "                chat_history_string = ''.join(f.readlines())\n",
        "            print(\"Existing chat history loaded.\")\n",
        "    else:\n",
        "        # Handle the case where the file is not found\n",
        "        open(chat_history_file, 'w').close()\n",
        "        chat_history_string = \"\"\n",
        "        print('chat_history.txt created to save the conversation history.')\n",
        "\n",
        "    print('\\n------------------------------------------------------------------------------------------------------------------------\\n')\n",
        "\n",
        "    while True:\n",
        "        with open(chat_history_file, 'r') as f:\n",
        "            # Read all lines and join them into a single string\n",
        "            chat_history_string = ''.join(f.readlines())\n",
        "\n",
        "        user_prompt = input('Prompt: ')\n",
        "        print('\\n------------------------------------------------------------------------------------------------------------------------\\n')\n",
        "\n",
        "        if user_prompt == 'exit' or user_prompt == 'q' or user_prompt == 'quit':\n",
        "            api_key='' # Provide your API key here\n",
        "            # Call the function to calculate daily usage\n",
        "            daily_usage = calculate_daily_usage(api_key)\n",
        "\n",
        "            if daily_usage is not None:\n",
        "                print(f\"Used balance in dollars for today: ${daily_usage:.6f}\")\n",
        "\n",
        "            break\n",
        "\n",
        "        system_role = f\"You are a market analyzer that reviews all of the text provided to you which may include customer feedbacks and reviews, sales data, price data, etc from a business. Then you should analyze that business, it's strength and weaknesses, analyzes the market and the gaps and needs in that market. Then having that data in memory, wait for the user prompt to analyze the business in the provided data according to the user prompt. \\nHere is the provided data: \\n{dataset}. Also, here is the chat history so far: {chat_history_string}.\"\n",
        "\n",
        "        user_role = f\"Here is the prompt: \\n{user_prompt}.\"\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            #model=\"gpt-3.5-turbo\",\n",
        "            model=\"gpt-4-1106-preview\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_role},\n",
        "                {\"role\": \"user\", \"content\": user_role}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        print('\\033[1mAnalyzers Output:\\033[0m \\n')\n",
        "        prompt_output = print_plain_text(completion.choices[0].message)\n",
        "        print('\\n------------------------------------------------------------------------------------------------------------------------\\n')\n",
        "\n",
        "        # Store prompt and output\n",
        "        prompts_and_outputs[user_prompt] = prompt_output\n",
        "\n",
        "        # Append prompt and response to chat history\n",
        "        with open(chat_history_file, 'a') as f:\n",
        "            f.write(f'Prompt: {user_prompt}\\nAnswer: {prompt_output}\\n')\n"
      ],
      "metadata": {
        "id": "3LbE_oq4U124"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet showcases how to calculate and display the daily usage cost in dollars for the OpenAI GPT-4 model. Key features include API key integration, usage data retrieval, and concise output of the daily cost. Users can easily monitor their OpenAI GPT-4 usage and associated expenses:"
      ],
      "metadata": {
        "id": "nbbIUFKvRhsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide your API key here\n",
        "api_key = \"\"\n",
        "\n",
        "# Call the function to calculate daily usage\n",
        "daily_usage = calculate_daily_usage(api_key)\n",
        "\n",
        "if daily_usage is not None:\n",
        "    print(f\"Used balance in dollars for today: ${daily_usage:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iULXfm-Km44q",
        "outputId": "6cad67c1-4ce1-4e18-a296-2cec88695be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used balance in dollars for today: $0.098406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Code"
      ],
      "metadata": {
        "id": "tnMaYObpJzrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code segment serves a dual purpose: it enables users to upload a dataset file for analysis and subsequently trims the dataset to a specified maximum token limit using the NLTK library. By employing the upload_file function, users can conveniently bring in datasets in various formats, and the trim_tokens_nltk function ensures that the dataset does not exceed a specified token threshold for effective processing with the ChatGPT context window limitations:"
      ],
      "metadata": {
        "id": "50wms1hgRtXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the function to upload and process the file\n",
        "dataset = upload_file()\n",
        "dataset = trim_tokens_nltk(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "a149431d-a306-493f-831c-33ce2a3e619a",
        "id": "zfdq77eMJzrU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please select \"Choose File\" to select a dataset from your hard disk for upload and analysis: \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8d9deb6d-8514-45cb-9fff-9d5c484b5d31\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8d9deb6d-8514-45cb-9fff-9d5c484b5d31\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test_yourmechanic_dataset.xlsx to test_yourmechanic_dataset.xlsx\n",
            "Uploaded file: test_yourmechanic_dataset.xlsx\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Do you want to consider a single column of the dataset?\n",
            "Type \"yes\" if you want to choose a single column, and type \"no\" if you wish to use the entire dataset:\n",
            "yes\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Please type the title of the column you wish to consider for analysis:\n",
            "review-text\n",
            "\n",
            "Dataset uploaded and column 'review-text' is selected for analysis.\n",
            "\n",
            "Estimated number of tokens: 9796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code initiates the main loop for a chat analysis system. It involves providing the OpenAI API key, specifying the chat history file path, and calling the main_loop function. Within this loop, users interact by inputting prompts related to a dataset, and the system generates responses for analysis. Additionally, it manages the chat history, allowing users to decide whether to delete previous chat data for each session:"
      ],
      "metadata": {
        "id": "SNW5uB_CSAws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = '' # Provide your API key here\n",
        "chat_history_file = \"chat_history.txt\"  # Replace with the actual file path\n",
        "\n",
        "# Call the main loop function\n",
        "main_loop(api_key, chat_history_file, dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guddulFlAttC",
        "outputId": "2d61f41f-6cdf-4482-c38c-dc49c74b741b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m------------------------------------------------------------------------------------------------------------------------\u001b[0m \n",
            "\n",
            "\u001b[1mUser Guide\u001b[0m\n",
            "1) Please type your prompt to ask questions about the dataset.\n",
            "2) You can use \"q\", \"quit\", or \"exit\" commands to stop the code from asking for prompts.\n",
            "3) If asked for deleting the previous chat history or not, typing \"yes\" will delete it and then the model\n",
            "   will not consider previous chat data in its answers. Otherwise, type \"no\" for using the previous chat data.\n",
            "   (These words are case-sensitive)\n",
            "\n",
            "\u001b[1m------------------------------------------------------------------------------------------------------------------------\u001b[0m \n",
            "\n",
            "chat_history.txt created to save the conversation history.\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: what is the data about?\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mAnalyzers Output:\u001b[0m \n",
            "\n",
            "The data provided is about customer experiences with a mobile mechanic service, which is presumably called \"Your\n",
            "Mechanic\" or could alternatively be associated with \"Wrench/YourMechanic.\" The service seems to involve mechanics\n",
            "visiting customers' locations to perform automotive repairs and maintenance.\n",
            "The feedback contains dates of experience, details about the mechanics' punctuality, professionalism, knowledge, and the\n",
            "quality of work performed. The testimonials are mixed, with many customers expressing satisfaction regarding the\n",
            "convenience of the service, the mechanics' expertise, and the transparency in pricing. However, there are also several\n",
            "instances of negative experiences, where customers complain about misdiagnoses, poor communication, last-minute\n",
            "rescheduling, and issues with pricing and quality of work.\n",
            "Key aspects mentioned across multiple reviews include:\n",
            "\u001b[1m•\u001b[0m On-site service at the customers' preferred locations (commonly viewed as convenient). - Availability and punctuality\n",
            "of the mechanics, with many experiencing on-time or early arrivals, but also some instances of tardiness or\n",
            "rescheduling. - Professionalism and friendliness of the mechanics, which is frequently praised. - Quality of work, with\n",
            "several customers noting successful repairs but others mentioning unresolved problems, misdiagnoses, or even worsened\n",
            "conditions after the service. - Pricing concerns, with some customers finding the service transparent and reasonable,\n",
            "while others criticize it for being misleading or overpriced. - Customer service communication, which is both lauded for\n",
            "efficiency and follow-up, and criticized for lack of communication, especially when there are issues with the service.\n",
            "This summary covers the broad themes and trends within the feedback, indicating a service that has room to improve in\n",
            "consistency and reliability, yet also offers convenience and expertise that some customers greatly appreciate.\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: what are the strengths of the business?\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mAnalyzers Output:\u001b[0m \n",
            "\n",
            "Based on the provided data, the strengths of Your Mechanic or Wrench/YourMechanic mobile mechanic service include:\n",
            "\u001b[1m1.\u001b[0m \u001b[1mConvenience:\u001b[0m    - The business model offers on-site service where mechanics go directly to the customer's\n",
            "location, which customers find convenient since it saves them from having to take their vehicle to a shop.\n",
            "\u001b[1m2.\u001b[0m \u001b[1mProfessional and Friendly Mechanics:\u001b[0m    - Many customers have reported that the mechanics are professional,\n",
            "friendly, and courteous. Positive personality traits and professionalism go a long way in customer satisfaction and\n",
            "repeat business.\n",
            "\u001b[1m3.\u001b[0m \u001b[1mKnowledgeable Staff:\u001b[0m    - There are numerous mentions of mechanics being knowledgeable and capable of diagnosing\n",
            "issues quickly and accurately. Customer reviews often praise the expertise of mechanics sent by the service.\n",
            "\u001b[1m4.\u001b[0m \u001b[1mTimeliness:\u001b[0m    - Several customers appreciated the punctuality of mechanics, with some mechanics even arriving\n",
            "earlier than the scheduled time, leading to a positive service experience.\n",
            "\u001b[1m5.\u001b[0m \u001b[1mTransparency and Upfront Pricing:\u001b[0m    - Some rapport was built on the notion of transparency in pricing, as a few\n",
            "customers were pleased with the pricing being as advertised or having upfront quotes.\n",
            "\u001b[1m6.\u001b[0m \u001b[1mEase of Scheduling:\u001b[0m    - The application or website is easy to use for scheduling service appointments, as\n",
            "indicated by customer feedback.\n",
            "\u001b[1m7.\u001b[0m \u001b[1mCommunication:\u001b[0m    - The business has a proactive communication strategy with reminders and updates sent to\n",
            "customers, which some clients appreciated for keeping them informed.\n",
            "\u001b[1m8.\u001b[0m \u001b[1mQuality of Work:\u001b[0m    - When work is done correctly, customers have expressed high satisfaction and a perception of\n",
            "good value for the service provided.\n",
            "\u001b[1m9.\u001b[0m \u001b[1mComprehensive Service Range:\u001b[0m    - The reviews reflect a wide range of offered services, from battery\n",
            "installations and oil changes to more complex repairs, showing that the business can cater to various customer needs.\n",
            "These strengths indicate that the business is well-received in many cases, especially for customers valuing convenience\n",
            "and professional service at their doorstep. The business's ability to maintain these facets can serve as a foundation\n",
            "for customer loyalty and positive word-of-mouth referrals.\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: what are the main objectives of the business? (as bullet points)\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mAnalyzers Output:\u001b[0m \n",
            "\n",
            "Based on the provided data, the main objectives of the mobile mechanic service, potentially identified as \"Your\n",
            "Mechanic\" or \"Wrench/YourMechanic,\" seem to be:\n",
            "\u001b[1m•\u001b[0m \u001b[1mProviding Convenient Automotive Services:\u001b[0m     To offer customers the convenience of car repairs and maintenance\n",
            "services conducted at their home or office location.\n",
            "\u001b[1m•\u001b[0m \u001b[1mEnsuring Customer Satisfaction:\u001b[0m     Strive for high levels of customer satisfaction through professional,\n",
            "friendly, and knowledgeable service.\n",
            "\u001b[1m•\u001b[0m \u001b[1mTimely and Reliable Service Delivery:\u001b[0m     Ensure the mechanics are punctual for appointments and efficient in\n",
            "delivering the required service within a reasonable time frame.\n",
            "\u001b[1m•\u001b[0m \u001b[1mTransparent and Competitive Pricing:\u001b[0m     Offer competitive and transparent pricing that provides value to the\n",
            "customer and matches the quality of service delivered.\n",
            "\u001b[1m•\u001b[0m \u001b[1mEase of Booking and Scheduling:\u001b[0m     Provide an easy-to-use platform (app/website) for customers to schedule\n",
            "services at their convenience.\n",
            "\u001b[1m•\u001b[0m \u001b[1mOffering a Broad Range of Services:\u001b[0m     Provide a wide array of automotive repair and maintenance services to\n",
            "cater to various customer needs and vehicle issues.\n",
            "\u001b[1m•\u001b[0m \u001b[1mMaintaining Effective Communication:\u001b[0m     Keep customers informed and updated before, during, and after the service\n",
            "to foster trust and transparency.\n",
            "\u001b[1m•\u001b[0m \u001b[1mBuilding a Base of Skilled and Professional Mechanics:\u001b[0m     Recruit and deploy competent mechanics who can\n",
            "accurately diagnose, explain, and resolve vehicle problems.\n",
            "\u001b[1m•\u001b[0m \u001b[1mExpand Customer Base and Market Reach:\u001b[0m     Grow the business by acquiring new customers through positive referrals\n",
            "and maintaining a high standard of service to encourage repeat business.\n",
            "These objectives focus on delivering an automobile service that is not only technically proficient but also places a\n",
            "strong emphasis on customer experience and convenience.\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: please explain more about the third objective you just mentioned\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[1mAnalyzers Output:\u001b[0m \n",
            "\n",
            "The third objective I mentioned, \"Timely and Reliable Service Delivery,\" revolves around the concept that the business\n",
            "aims to provide its mobile mechanic services in a manner that is both prompt and dependable. This objective likely\n",
            "consists of several key components and strategies:\n",
            "\u001b[1m•\u001b[0m \u001b[1mPunctuality of Mechanics:\u001b[0m   Ensuring that mechanics arrive at the customer's designated location at the scheduled\n",
            "time, if not earlier. Punctuality is often cited by customers as a positive aspect of their service experience,\n",
            "enhancing the overall reputation of the business.\n",
            "\u001b[1m•\u001b[0m \u001b[1mEfficient Service Execution:\u001b[0m   Mechanics are expected to work swiftly without compromising the quality of their\n",
            "workmanship. Completing tasks within a reasonable and expected timeframe shows respect for the customer's schedule.\n",
            "\u001b[1m•\u001b[0m \u001b[1mReliability and Trustworthiness:\u001b[0m   The objective implies building trust with customers by providing consistent,\n",
            "high-quality service. Customers need to feel confident that the mechanics can handle a variety of automotive issues\n",
            "reliably and that the repairs or maintenance performed will be lasting and effective.\n",
            "\u001b[1m•\u001b[0m \u001b[1mMinimizing Rescheduling and Delays:\u001b[0m   Strive to reduce the occurrence of last-minute rescheduling and delays.\n",
            "Adequate planning and inventory management can help prevent cancellation or rescheduling due to missing parts or\n",
            "scheduling conflicts.\n",
            "\u001b[1m•\u001b[0m \u001b[1mEffective Job Planning and Allocation:\u001b[0m   Implement a robust system for scheduling and dispatching mechanics based\n",
            "on their expertise, availability, and proximity to the customer's location, ensuring that the right mechanic is\n",
            "available at the right time.\n",
            "\u001b[1m•\u001b[0m \u001b[1mFollow-Up and Quality Assurance:\u001b[0m   After the completion of a job, follow-up communication may be used to confirm\n",
            "customer satisfaction and address any lingering issues, ensuring that customers can depend on the service for any\n",
            "follow-up work required.\n",
            "This objective is vital because it directly impacts customer satisfaction and loyalty. When a company consistently meets\n",
            "or exceeds expectations for timeliness and reliability, it is likely to see more repeat business, more positive reviews,\n",
            "and a strong overall brand image.\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Prompt: q\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Used balance in dollars for today: $0.097946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9-0jAsAeJzrW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}